{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlv6knX04FiY"
   },
   "source": [
    "# Link to Original Git Repository\n",
    "https://github.com/pranavsinghps1/CASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link to Our Git Repository \n",
    "https://github.com/TonyDeng1997/CASS_UIUC598DLH\n",
    "\n",
    "It contains the checkpoints to be used with our model as well as all the required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPrIEzfXdsZJ"
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1DOgSQ6zdzDG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops==0.4.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: matplotlib==3.5.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.5.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: numpy==1.23.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.23.1)\n",
      "Requirement already satisfied: pandas==1.4.3 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.4.3)\n",
      "Requirement already satisfied: Pillow==9.2.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (9.2.0)\n",
      "Requirement already satisfied: scikit-learn==1.1.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: scipy==1.8.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard==2.9.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.9.1)\n",
      "Requirement already satisfied: timm==0.5.4 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.5.4)\n",
      "Requirement already satisfied: torch==1.11.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.11.0)\n",
      "Requirement already satisfied: torchaudio==0.11.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: torchcontrib==0.0.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.0.2)\n",
      "Requirement already satisfied: torchmetrics==0.9.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.9.2)\n",
      "Requirement already satisfied: torchvision==0.12.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.12.0)\n",
      "Requirement already satisfied: vit-pytorch==0.35.8 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (0.35.8)\n",
      "Requirement already satisfied: pytorch-lightning==1.6.5 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (1.6.5)\n",
      "Requirement already satisfied: tqdm==4.64.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (24.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (4.51.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib==3.5.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: traitlets in /home/hdeng11/.local/lib/python3.10/site-packages (from matplotlib-inline==0.1.2->-r requirements.txt (line 3)) (5.14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from pandas==1.4.3->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-learn==1.1.1->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-learn==1.1.1->-r requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (2.31.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (3.0.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (1.62.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (3.19.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (69.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard==2.9.1->-r requirements.txt (line 9)) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions in /home/hdeng11/.local/lib/python3.10/site-packages (from torch==1.11.0->-r requirements.txt (line 11)) (4.11.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (2024.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/lib/python3/dist-packages (from pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (5.4.1)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (0.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/hdeng11/.local/lib/python3.10/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (3.9.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 9)) (5.3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/hdeng11/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 9)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/hdeng11/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 9)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 9)) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard==2.9.1->-r requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.9.1->-r requirements.txt (line 9)) (2.1.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5->-r requirements.txt (line 17)) (23.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/hdeng11/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.9.1->-r requirements.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.9.1->-r requirements.txt (line 9)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/MedMNIST/MedMNIST.git\n",
      "  Cloning https://github.com/MedMNIST/MedMNIST.git to /tmp/pip-req-build-0clm6k16\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MedMNIST/MedMNIST.git /tmp/pip-req-build-0clm6k16\n",
      "  Resolved https://github.com/MedMNIST/MedMNIST.git to commit db5bff9d0faef4d273896e3cb5542a7000c0239f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (1.23.1)\n",
      "Requirement already satisfied: pandas in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (1.4.3)\n",
      "Requirement already satisfied: scikit-learn in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (1.1.1)\n",
      "Requirement already satisfied: scikit-image in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (0.22.0)\n",
      "Requirement already satisfied: tqdm in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (4.64.0)\n",
      "Requirement already satisfied: Pillow in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (9.2.0)\n",
      "Requirement already satisfied: fire in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (0.6.0)\n",
      "Requirement already satisfied: torch in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /home/hdeng11/.local/lib/python3.10/site-packages (from medmnist==3.0.1) (0.12.0)\n",
      "Requirement already satisfied: termcolor in /home/hdeng11/.local/lib/python3.10/site-packages (from fire->medmnist==3.0.1) (2.4.0)\n",
      "Requirement already satisfied: six in /home/hdeng11/.local/lib/python3.10/site-packages (from fire->medmnist==3.0.1) (1.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from pandas->medmnist==3.0.1) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from pandas->medmnist==3.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (1.8.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (2024.2.12)\n",
      "Requirement already satisfied: packaging>=21 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (24.0)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (2.34.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (0.4)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-image->medmnist==3.0.1) (3.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-learn->medmnist==3.0.1) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hdeng11/.local/lib/python3.10/site-packages (from scikit-learn->medmnist==3.0.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/hdeng11/.local/lib/python3.10/site-packages (from torch->medmnist==3.0.1) (4.11.0)\n",
      "Requirement already satisfied: requests in /home/hdeng11/.local/lib/python3.10/site-packages (from torchvision->medmnist==3.0.1) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests->torchvision->medmnist==3.0.1) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests->torchvision->medmnist==3.0.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests->torchvision->medmnist==3.0.1) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hdeng11/.local/lib/python3.10/site-packages (from requests->torchvision->medmnist==3.0.1) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade git+https://github.com/MedMNIST/MedMNIST.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "*   **Background of the problem:**\n",
    "\n",
    "  Medical image analysis is vitally important in diagnosing diseases and predicting patient outcomes in general.  However, the effectiveness of deep learning techniques in this realm is often hindered by a lack of labeled data due to issues such as a need for the domain-specific knowledge required for labeling, patient privacy concerns, disease prevalence, and an incomplete understanding of rare or emerging diseases.  This is a challenge when applying deep learning techniques whose performance often relies on large annotated datasets.  Additionally, existing state-of-the-art self-supervised learning techniques often require significant computational resources, such as large batch sizes and extensive training times, to achieve maximum performance.  This oftentimes makes them impractical for many medical imaging applications where compute budgets are limited.\n",
    "\n",
    "  Current state-of-the-art self-supervised models such as DINO and BYOL can perform well with limited labeled data, but they still require considerable computational resources.  DINO is a teacher and student network, where both the student and the teacher utilize a Vision Transformer (ViT) architecture.  This state-of-the-art, self-supervised model performed comparatively well to its predecessors when trained on unlabeled data.  However, its performance is sometimes dwarfed by supervised models.  Developing a deep learning architecture which performs well with limited labeled data and limited computational resources, could lead to increased patient longevity and general health.  \n",
    "\n",
    "*   **Paper explanation:**\n",
    "\n",
    "  The key idea proposed in the CASS paper [1] is to leverage both CNNs and Transformers simultaneously, in a self-supervised learning approach, in order to address the challenges of limited labeled data and high computational cost in medical image analysis.  Unlike existing self-supervised methods, which may use one of these architectures, CASS passes images through a CNN and Transformer in parallel to extract their representations.  These representations are then used to find cosine similarity loss.  The idea is that CNNs and Transformers have different strong suits and by training them in parallel theyâ€™re able to learn from one another.  CNNs are translation equivariant and better at capturing local details, while Transformers are better at modeling global context.  By contrasting the features extracted by each architecture, they are able to learn from one another and eventually capture a richer and more useful representation.\n",
    "\n",
    "  The authors demonstrate that CASS outperforms existing self-supervised methods, like DINO, in terms of F1 Score and Recall value by an average of 3.8% with 1% labeled data, 5.9% with 10% labeled data, and 10.13% with 100% labeled data [1].  It achieved this while requiring 69% less training time on average, compared to DINO.  CASS was also shown to be more robust to changes in batch size and training epochs, which is a key limitation in compute restricted environments.  \n",
    "\n",
    "  CASS marks an important step towards making self-supervised deep learning models more widely accessible and practically useful for medical image analysis.  It has the potential to accelerate research on rare and emerging diseases where labeled data is scarce, thus advancing the field of computer-aided diagnosis and patient outcome prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "In our project, we aim to validate the following hypotheses:\n",
    "\n",
    "* **Hypothesis 1:** CASS requires less training time than existing self-supervised models (DINO).\n",
    "\n",
    "* **Hypothesis 2:** CASS is more robust to changes in batch size and number of epochs.\n",
    "\n",
    "To test these hypotheses, we will conduct the following experiments:\n",
    "\n",
    "\n",
    "1.   **Training time comparison:** We will measure and compare the time required to train CASS and DINO on the same dataset.  We will be comparing our results to the DINO results presented in the paper   Unfortunately, our hardware will not exactly match what was used in the paper, so we will need to make a calculated estimation on what the expected difference would be given the differences between our hardware.\n",
    "\n",
    "2.   **Batch size ablation:** We will evaluate the performance change of CASS when pre-trained with different batch sizes on the same dataset.  We hope to test with batch sizes of 8, 16, and 32, though we may be limited by our hardware to do less.  We will measure the classification performance and compare results between the varying batch sizes.  The paper found that CASS actually performed better with smaller batch sizes and it will be interesting to see if this is reproduced in our own experiments.\n",
    "\n",
    "3.  **Pre-training epochs ablation:**  We will measure the effect of the number of pre-training epochs on the performance of CASS when trained on identical datasets.  Our goal is to pre-train with epochs of 50, 100, and 200.  As in experiment 2, we will measure the classification performance and compare results between the varying epoch ranges.  The authors found that there were diminishing returns on performance when increasing the epochs beyond 200, so we will focus our experiments on the lower epoch ranges.\n",
    "\n",
    "We have less computational resources than the authors did and our experiments may be affected.  Because of this, we plan to be flexible in our work and will update our experiments accordingly.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "The following section imports the required packages to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdeng11/.local/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v3.0.1 @ https://github.com/MedMNIST/MedMNIST/\n",
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# import  packages you need\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "TYS7rMxYx0K1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms as tsfm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "#from skimage.io import imread\n",
    "from os import listdir\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dataset we used is https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset as referred as brain tumor mri dataset in the paper\n",
    "* The detailed statistics are in the following section, it provided traning and testing split for us\n",
    "* The dataset didn't provide a label csv file, we'll generate label csv file for both training and testing datasets in this section as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_base_path = \"BrainMRI/Training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_base_path = \"BrainMRI/Testing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "training_folders = listdir(training_base_path) \n",
    "print(len(training_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glioma', 'meningioma', 'pituitary', 'notumor']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_count=[]\n",
    "for i in training_folders:\n",
    "    path = training_base_path + i\n",
    "    sub_files = listdir(path)\n",
    "    training_file_count.append(len(sub_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "testing_folders = listdir(testing_base_path) \n",
    "print(len(testing_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glioma', 'meningioma', 'pituitary', 'notumor']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "306\n",
      "300\n",
      "405\n"
     ]
    }
   ],
   "source": [
    "testing_file_count=[]\n",
    "for i in testing_folders:\n",
    "    path = testing_base_path + i\n",
    "    sub_files = listdir(path)\n",
    "    testing_file_count.append(len(sub_files))\n",
    "    print(len(sub_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainMRI/Training/glioma\n",
      "BrainMRI/Training/meningioma\n",
      "BrainMRI/Training/pituitary\n",
      "BrainMRI/Training/notumor\n",
      "Total Number of Training Images:5712\n"
     ]
    }
   ],
   "source": [
    "total_training_images = 0\n",
    "for n in range(len(training_folders)):\n",
    "    patient_id = training_folders[n]\n",
    "    patient_path = training_base_path  + patient_id\n",
    "    print(patient_path)\n",
    "    class_path = patient_path + \"/\"  + \"/\"\n",
    "    subfiles = listdir(class_path)\n",
    "    total_training_images += len(subfiles)\n",
    "print(\"Total Number of Training Images:\" + str(total_training_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainMRI/Testing/glioma\n",
      "BrainMRI/Testing/meningioma\n",
      "BrainMRI/Testing/pituitary\n",
      "BrainMRI/Testing/notumor\n",
      "Total Number of Testing Images:1311\n"
     ]
    }
   ],
   "source": [
    "total_testing_images = 0\n",
    "for n in range(len(testing_folders)):\n",
    "    patient_id = testing_folders[n]\n",
    "    patient_path = testing_base_path  + patient_id\n",
    "    print(patient_path)\n",
    "    class_path = patient_path + \"/\"  + \"/\"\n",
    "    subfiles = listdir(class_path)\n",
    "    total_testing_images += len(subfiles)\n",
    "print(\"Total Number of Testing Images:\" + str(total_testing_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 306, 300, 405]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1008 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(ncols=3, figsize=(20, 14))\n",
    "\n",
    "# Plotting training data types\n",
    "\n",
    "ax[0].set_title('Training Data')\n",
    "ax[0].pie(\n",
    "    training_file_count,\n",
    "    labels=training_folders,\n",
    "    colors=['#FAC500','#0BFA00', '#0066FA','#FA0000'], \n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p *total_training_images / 100),\n",
    "    explode=(0.01, 0.01, 0.1, 0.01),\n",
    "    textprops={'fontsize': 20}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_title('Train Test Split')\n",
    "ax[1].pie(\n",
    "    [total_training_images, total_testing_images],\n",
    "    labels=['Train','Test'],\n",
    "    colors=['darkcyan', 'orange'], \n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * sum([total_training_images, total_testing_images]) / 100),\n",
    "    explode=(0.1, 0),\n",
    "    startangle=85,\n",
    "    textprops={'fontsize': 20}\n",
    ")\n",
    "\n",
    "\n",
    "ax[2].set_title('Testing Data')\n",
    "ax[2].pie(\n",
    "    testing_file_count,\n",
    "    labels=testing_folders,\n",
    "    colors=['#FAC500', '#0BFA00', '#0066FA', '#FA0000'],\n",
    "    autopct=lambda p: '{:.2f}%\\n{:,.0f}'.format(p, p * total_testing_images / 100),\n",
    "    explode=(0.01, 0.01, 0.1, 0.01),\n",
    "    textprops={'fontsize': 20} \n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BrainMRI/Training/glioma/Tr-gl_0162.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BrainMRI/Training/glioma/Tr-gl_0840.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BrainMRI/Training/glioma/Tr-gl_0372.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BrainMRI/Training/glioma/Tr-gl_0343.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BrainMRI/Training/glioma/Tr-gl_0367.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path  target\n",
       "0  BrainMRI/Training/glioma/Tr-gl_0162.jpg  glioma\n",
       "1  BrainMRI/Training/glioma/Tr-gl_0840.jpg  glioma\n",
       "2  BrainMRI/Training/glioma/Tr-gl_0372.jpg  glioma\n",
       "3  BrainMRI/Training/glioma/Tr-gl_0343.jpg  glioma\n",
       "4  BrainMRI/Training/glioma/Tr-gl_0367.jpg  glioma"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(index=np.arange(0, total_training_images), columns=[\"path\", \"target\"])\n",
    "\n",
    "k = 0\n",
    "for n in range(len(training_folders)):\n",
    "    patient_id = training_folders[n]\n",
    "    patient_path = training_base_path  + patient_id\n",
    "    class_path = patient_path + \"/\"\n",
    "    subfiles = listdir(class_path)\n",
    "    for m in range(len(subfiles)):\n",
    "        image_path = subfiles[m]\n",
    "        data.iloc[k][\"path\"] = class_path + image_path \n",
    "        data.iloc[k][\"target\"] = patient_id\n",
    "        k += 1  \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notumor       0.279237\n",
       "pituitary     0.255077\n",
       "meningioma    0.234419\n",
       "glioma        0.231268\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If would like to take a subset of data, do it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.sample(int(len(data)*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4098     pituitary\n",
       "871         glioma\n",
       "3503     pituitary\n",
       "1765    meningioma\n",
       "2334    meningioma\n",
       "           ...    \n",
       "1317        glioma\n",
       "2283    meningioma\n",
       "2004    meningioma\n",
       "3668     pituitary\n",
       "607         glioma\n",
       "Name: target, Length: 5712, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating CSV for the entire dataset\n",
    "data_train.to_csv('BrainMRI/training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the class weights to use with focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pituitary': 0, 'glioma': 1, 'meningioma': 2, 'notumor': 3}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_str2num={}\n",
    "num=0\n",
    "for i in data_train['target'].unique():\n",
    "    label_str2num[i]=num\n",
    "    num+=1\n",
    "label_str2num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for i in data_train['target']:\n",
    "    if i in data:\n",
    "        data[i]+=1\n",
    "    else:\n",
    "        data[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notumor       1595\n",
       "pituitary     1457\n",
       "meningioma    1339\n",
       "glioma        1321\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data={}\n",
    "for i in data:\n",
    "    new_data[label_str2num[i]]=data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1457, 1: 1321, 2: 1339, 3: 1595}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dist = OrderedDict(sorted(new_data.items()))\n",
    "dist=dict(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array =  dict_values([1457, 1321, 1339, 1595])\n",
      "Normalized Array =  [0.5970802919708029, 0.2, 0.25255474452554744, 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize(arr, t_min, t_max):\n",
    "    norm_arr = []\n",
    "    diff = t_max - t_min\n",
    "    diff_arr = max(arr) - min(arr)\n",
    "    for i in arr:\n",
    "        temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "        norm_arr.append(temp)\n",
    "    return norm_arr\n",
    "  \n",
    "# assign array and range\n",
    "array_1d = dist.values()\n",
    "range_to_normalize = (0.2, 1)\n",
    "normalized_array_1d = normalize(\n",
    "    array_1d, range_to_normalize[0], \n",
    "  range_to_normalize[1])\n",
    "  \n",
    "# display original and normalized array\n",
    "print(\"Original Array = \", array_1d)\n",
    "print(\"Normalized Array = \", normalized_array_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same to generate csv file for test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BrainMRI/Testing/glioma/Te-gl_0262.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BrainMRI/Testing/glioma/Te-gl_0061.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BrainMRI/Testing/glioma/Te-gl_0040.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BrainMRI/Testing/glioma/Te-gl_0287.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BrainMRI/Testing/glioma/Te-gl_0075.jpg</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     path  target\n",
       "0  BrainMRI/Testing/glioma/Te-gl_0262.jpg  glioma\n",
       "1  BrainMRI/Testing/glioma/Te-gl_0061.jpg  glioma\n",
       "2  BrainMRI/Testing/glioma/Te-gl_0040.jpg  glioma\n",
       "3  BrainMRI/Testing/glioma/Te-gl_0287.jpg  glioma\n",
       "4  BrainMRI/Testing/glioma/Te-gl_0075.jpg  glioma"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(index=np.arange(0, total_testing_images), columns=[\"path\", \"target\"])\n",
    "\n",
    "k = 0\n",
    "for n in range(len(testing_folders)):\n",
    "    patient_id = testing_folders[n]\n",
    "    patient_path = testing_base_path  + patient_id\n",
    "    class_path = patient_path + \"/\"\n",
    "    subfiles = listdir(class_path)\n",
    "    for m in range(len(subfiles)):\n",
    "        image_path = subfiles[m]\n",
    "        data.iloc[k][\"path\"] = class_path + image_path \n",
    "        data.iloc[k][\"target\"] = patient_id\n",
    "        k += 1  \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        glioma\n",
       "1        glioma\n",
       "2        glioma\n",
       "3        glioma\n",
       "4        glioma\n",
       "         ...   \n",
       "1306    notumor\n",
       "1307    notumor\n",
       "1308    notumor\n",
       "1309    notumor\n",
       "1310    notumor\n",
       "Name: target, Length: 1311, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('BrainMRI/testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "This model configures the CASS pretraining algorithm with furthur finetuning with both CNN and Vit. We are reusing the code from here https://github.com/pranavsinghps1/CASS/blob/master/CASS.ipynb and reusing parameters used in paper for draft purpose with minor changes due to different datasets were being used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are interested in the result and would not like to run the training again, please jump to the result section, we've saved a checkpoint to use in the result section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tsfm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the label_num2str and cls weight to here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # data path\n",
    "    data_path  = 'BrainMRI/training.csv'\n",
    "    train_imgs_dir  = 'BrainMRI/Training'\n",
    "    # model info\n",
    "    # label info\n",
    "    label_num2str = {0: 'glioma',\n",
    "                     1: 'pituitary',\n",
    "                     2:'notumor',\n",
    "                     3:'meningioma'\n",
    "                     }\n",
    "    label_str2num = {'glioma': 0,\n",
    "                     'pituitary':1,\n",
    "                     'notumor':2,\n",
    "                     'meningioma':3\n",
    "                     }\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight =  [0.2, 0.5970802919708029, 1.0, 0.25255474452554744] # copy the cls_weight from previous step or just use the variable\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_384'\n",
    "    seed = 77\n",
    "    num_classes = 4\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 77\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(77)\n",
    "cfg=CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Image Mean and Variance to be used for transform function, since we are using the same dataset. We don't really have to run this multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),  # Resize all images to the same size\n",
    "    transforms.ToTensor()           # Transform images to PyTorch tensors\n",
    "])\n",
    "dataset = datasets.ImageFolder('BrainMRI/Training', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842268672\n",
      "Mean: tensor([0.1857, 0.1857, 0.1858])\n",
      "Std Deviation: tensor([0.2018, 0.2018, 0.2018])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_mean_and_std(dataloader):\n",
    "    channel_sum, channel_squared_sum, num_elements = 0, 0, 0\n",
    "\n",
    "    for data, _ in dataloader:\n",
    "        # Reshape data to be the shape of [B, C, W*H]\n",
    "        data = data.view(data.size(0), data.size(1), -1)\n",
    "        # Sum over all pixels and all batches for each channel\n",
    "        channel_sum += torch.sum(data, dim=[0, 2])\n",
    "        channel_squared_sum += torch.sum(data ** 2, dim=[0, 2])\n",
    "        num_elements += data.size(2) * data.size(0)\n",
    "\n",
    "    # Calculate the mean and variance\n",
    "    print(num_elements)\n",
    "    mean = channel_sum / num_elements\n",
    "    variance = (channel_squared_sum / num_elements) - (mean ** 2)\n",
    "    std_dev = torch.sqrt(variance)\n",
    "\n",
    "    return mean, std_dev\n",
    "\n",
    "# Calculate mean and std\n",
    "mean, std = get_mean_and_std(dataloader)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std Deviation: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define train & valid image transformation\n",
    "\"\"\"\n",
    "# The mean and std from previous step goes here\n",
    "DATASET_IMAGE_MEAN = (0.1857, 0.1857, 0.1858)\n",
    "DATASET_IMAGE_STD = (0.2018, 0.2018, 0.2018)\n",
    "\n",
    "train_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomAffine(degrees=10),], p=0.3),\n",
    "                                tsfm.RandomVerticalFlip(p=0.3),\n",
    "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n",
    "valid_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define dataset class\n",
    "\"\"\"\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, cfg, img_names: list, labels: list, transform=None):\n",
    "        self.img_dir = cfg.train_imgs_dir\n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_names[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_ts = self.transform(img)\n",
    "        label_ts = self.labels[idx]\n",
    "        return img_ts, label_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Focal-Loss\n",
    "\"\"\"\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        #To switch between F1 score and recall.\n",
    "        #f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        rec = self.tp/(self.tp + self.fn)\n",
    "        return rec\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(cfg.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['path'], df['target'], test_size=0.2, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_names: list = X_train.values.tolist()\n",
    "all_img_names_valid: list = X_val.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4569"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_names_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5712"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_names) + len(all_img_names_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_labels_ts = []\n",
    "for tmp_lb in y_train:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label_num=CFG.label_str2num.get(tmp_lb)\n",
    "    k=int(label_num)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_ts.append(tmp_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_labels_val_ts = []\n",
    "for tmp_lb in y_val:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label_num=CFG.label_str2num.get(tmp_lb)\n",
    "    k=int(label_num)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_val_ts.append(tmp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "model_cnn.to(device)\n",
    "model_vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
    "    writer = SummaryWriter()\n",
    "    phase = 'train'\n",
    "    model_cnn.train()\n",
    "    model_vit.train()\n",
    "    f1_score_cnn=0\n",
    "    f1_score_vit=0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            for img,_ in train_loader:\n",
    "                f1_score_cnn=0\n",
    "                f1_score_vit=0\n",
    "                img = img.to(device)\n",
    "                pred_vit = model_vit(img)\n",
    "                pred_cnn = model_cnn(img)\n",
    "                model_sim_loss=loss_fn(pred_vit,pred_cnn)\n",
    "                loss = model_sim_loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cnn.step()\n",
    "                optimizer_vit.step()\n",
    "                scheduler_cnn.step()\n",
    "                scheduler_vit.step()\n",
    "            print('For -',i,'Loss:',loss) \n",
    "            writer.add_scalar(\"Self-Supervised Loss/train\", loss, i)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr= 1e-3))\n",
    "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr= 1e-3))\n",
    "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "\n",
    "criterion_vit = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "criterion_cnn = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
    "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(77)\n",
    "x=0.1 #currently set to use 10% of the labels for reduced label training \n",
    "onep=random.sample(range(0, len(X_train)), int(len(X_train)*x))\n",
    "all_img_names_train = [all_img_names[idx] for idx in onep]\n",
    "all_img_labels_ts_train = [all_img_labels_ts[idx] for idx in onep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(CFG, all_img_names_train,all_img_labels_ts_train, train_transform)\n",
    "valid_dataset = Dataset(CFG, all_img_names_valid, all_img_labels_val_ts, valid_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssl_train_model will return two models (one CNN one Vit) for futurue fine tuning, while DINO only returns one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cov-T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#Train SSL\n",
    "print('Training Cov-T')\n",
    "#Change Epoche Here\n",
    "ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=50)\n",
    "#Saving SSL Models\n",
    "print('Saving Cov-T')\n",
    "torch.save(model_cnn,'./cass-r50-isic.pt')\n",
    "torch.save(model_vit,'./cass-r50-vit-isic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn=torch.load('cass-r50-isic.pt')\n",
    "model_vit=torch.load('cass-r50-vit-isic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tunning Cov-T\n",
      "Adjusting learning rate of group 0 to 3.0000e-04.\n",
      "tensor([[-0.2224, -0.0796, -0.0395,  0.0047],\n",
      "        [-0.2016, -0.0637, -0.0475, -0.0132],\n",
      "        [-0.2842, -0.0978, -0.0022, -0.0042],\n",
      "        [-0.1506, -0.0919,  0.0201, -0.0029],\n",
      "        [-0.2032, -0.0587, -0.0537, -0.0144],\n",
      "        [-0.1869, -0.0571, -0.0454, -0.0252],\n",
      "        [-0.2704, -0.0947,  0.0181,  0.0343],\n",
      "        [-0.2470, -0.0571, -0.0424, -0.0040],\n",
      "        [-0.1797, -0.0699, -0.0362, -0.0193],\n",
      "        [-0.2350, -0.0880,  0.0200, -0.0152],\n",
      "        [-0.2367, -0.0918, -0.0361, -0.0323],\n",
      "        [-0.1786, -0.0518, -0.0241, -0.0411],\n",
      "        [-0.2378, -0.0537, -0.0488, -0.0198],\n",
      "        [-0.2625, -0.1105,  0.0104, -0.0248],\n",
      "        [-0.2353, -0.0843, -0.0563, -0.0045],\n",
      "        [-0.1884, -0.0600, -0.0535, -0.0146]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.9713e-04.\n",
      "tensor([[-0.2923, -0.1407, -0.3052, -0.2684],\n",
      "        [-0.4407, -0.0278, -0.3867, -0.4032],\n",
      "        [-0.2888, -0.1699, -0.3380, -0.2741],\n",
      "        [-0.3111, -0.0465, -0.3216, -0.3530],\n",
      "        [-0.3601, -0.1308, -0.3760, -0.3521],\n",
      "        [-0.2935, -0.1751, -0.3503, -0.2644],\n",
      "        [-0.4614, -0.0347, -0.4062, -0.4100],\n",
      "        [-0.2804, -0.1245, -0.2536, -0.2571],\n",
      "        [-0.4838, -0.0581, -0.4041, -0.3926],\n",
      "        [-0.2956, -0.1631, -0.3425, -0.2655],\n",
      "        [-0.2976, -0.1425, -0.3327, -0.2985],\n",
      "        [-0.3182, -0.1158, -0.2790, -0.3249],\n",
      "        [-0.2886, -0.1721, -0.3595, -0.2553],\n",
      "        [-0.3173, -0.1108, -0.3267, -0.3305],\n",
      "        [-0.3009, -0.1525, -0.2990, -0.3071],\n",
      "        [-0.3857, -0.1401, -0.3280, -0.3435]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.8862e-04.\n",
      "tensor([[-0.3948, -0.2500, -0.5073, -0.7663],\n",
      "        [-0.3575, -0.2923, -0.4682, -0.7000],\n",
      "        [-0.5710, -0.2074, -0.3536, -0.6390],\n",
      "        [-0.1319, -0.1071, -0.1322, -0.2775],\n",
      "        [-0.4152, -0.2180, -0.4753, -0.6081],\n",
      "        [-0.3994, -0.2306, -0.5019, -0.7797],\n",
      "        [-0.4137, -0.2330, -0.3641, -0.7165],\n",
      "        [-0.3897, -0.1855, -0.4681, -0.5477],\n",
      "        [-0.4533, -0.0918, -0.4000, -0.5921],\n",
      "        [-0.4333, -0.1342, -0.3623, -0.5671],\n",
      "        [-0.3304, -0.1288, -0.3039, -0.4880],\n",
      "        [-0.4004, -0.2285, -0.4778, -0.7775],\n",
      "        [-0.3670, -0.2833, -0.4688, -0.7029],\n",
      "        [-0.4507, -0.2470, -0.3412, -0.8113],\n",
      "        [-0.3658, -0.2500, -0.4569, -0.7508],\n",
      "        [-0.3642, -0.1798, -0.3647, -0.6337]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.7480e-04.\n",
      "tensor([[-0.2662, -0.2587, -0.3572, -0.4150],\n",
      "        [-0.6134, -0.3329, -0.5444, -1.1773],\n",
      "        [-0.2911, -0.1722, -0.2080, -0.4563],\n",
      "        [-0.2964, -0.1690, -0.2964, -0.3847],\n",
      "        [-0.4076, -0.2530, -0.4141, -0.7080],\n",
      "        [-0.3821, -0.3358, -0.5522, -0.6768],\n",
      "        [-0.3566, -0.2378, -0.4168, -0.4976],\n",
      "        [-0.3342, -0.3496, -0.5732, -0.5608],\n",
      "        [-0.5227, -0.3614, -0.5144, -0.9873],\n",
      "        [-0.2169, -0.2244, -0.2937, -0.3260],\n",
      "        [-0.4923, -0.1848, -0.5535, -0.5243],\n",
      "        [-0.2264, -0.2831, -0.3964, -0.5277],\n",
      "        [-0.2486, -0.2427, -0.3285, -0.3642],\n",
      "        [-0.7652, -0.3903, -0.8725, -0.6416],\n",
      "        [-0.4456, -0.2867, -0.5927, -0.4200],\n",
      "        [-0.3068, -0.3001, -0.4632, -0.4359]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.5621e-04.\n",
      "tensor([[-0.6422, -0.5289, -0.3709, -0.9599],\n",
      "        [-0.7582, -0.4959, -0.6721, -0.8641],\n",
      "        [-0.4311, -0.1428, -0.3553, -0.5630],\n",
      "        [-0.0638, -0.1564, -0.2281, -0.3715],\n",
      "        [-0.4366, -0.4261, -0.5993, -0.3305],\n",
      "        [-0.1529, -0.2014, -0.2697, -0.1098],\n",
      "        [-0.2343, -0.2638, -0.3303, -0.1506],\n",
      "        [-0.1839, -0.2544, -0.3481, -0.1777],\n",
      "        [-0.0611, -0.1061, -0.1486, -0.1748],\n",
      "        [-1.1402, -0.6159, -1.1525, -1.0170],\n",
      "        [-0.5155, -0.3127, -0.2589, -0.8368],\n",
      "        [-0.2496, -0.2741, -0.3732, -0.1878],\n",
      "        [-0.0757, -0.1536, -0.2228, -0.2922],\n",
      "        [-0.2297, -0.2646, -0.3817, -0.1796],\n",
      "        [-0.1637, -0.1754, -0.2868, -0.1877],\n",
      "        [-0.8618, -0.4387, -0.4267, -1.1588]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.3356e-04.\n",
      "tensor([[-0.4038, -0.3359, -0.4730, -0.4515],\n",
      "        [-0.3237, -0.1534, -0.1698, -0.5448],\n",
      "        [-0.8480, -0.5261, -0.2865, -1.2341],\n",
      "        [-0.3478, -0.3141, -0.3228, -0.4108],\n",
      "        [-0.3441, -0.3420, -0.3961, -0.2814],\n",
      "        [-0.2357, -0.2026, -0.1731, -0.3544],\n",
      "        [-0.7444, -0.5353, -0.4136, -0.9286],\n",
      "        [-0.4993, -0.3860, -0.4870, -0.3409],\n",
      "        [-0.1973, -0.2060, -0.2667, -0.2216],\n",
      "        [-0.1015, -0.1718, -0.1880, -0.1056],\n",
      "        [-0.6544, -0.4917, -0.6224, -0.4747],\n",
      "        [-0.3352, -0.3329, -0.4780, -0.4494],\n",
      "        [-0.4437, -0.2688, -0.4379, -0.8660],\n",
      "        [-0.7107, -0.3139, -0.6788, -0.8415],\n",
      "        [-0.2931, -0.2786, -0.3978, -0.4460],\n",
      "        [-0.3774, -0.3418, -0.4214, -0.2920]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.0771e-04.\n",
      "tensor([[-1.0146, -0.5669, -0.4458, -1.4597],\n",
      "        [-0.5474, -0.3786, -0.5102, -0.5783],\n",
      "        [-0.4007, -0.3095, -0.3759, -0.3632],\n",
      "        [-0.4278, -0.3577, -0.4131, -0.5275],\n",
      "        [-0.5710, -0.3974, -0.5118, -0.6135],\n",
      "        [-0.0802, -0.0758, -0.0750, -0.2546],\n",
      "        [-0.5393, -0.4071, -0.4828, -0.5894],\n",
      "        [-0.5271, -0.3050, -0.2762, -0.7357],\n",
      "        [-0.4114, -0.1411, -0.2021, -0.6440],\n",
      "        [-0.3631, -0.3198, -0.3641, -0.4156],\n",
      "        [-0.6777, -0.2353, -0.0737, -1.2696],\n",
      "        [-0.2637, -0.2440, -0.2894, -0.2542],\n",
      "        [-0.3332, -0.1934, -0.3035, -0.5116],\n",
      "        [-0.0375, -0.0900, -0.0204, -0.2607],\n",
      "        [-0.5298, -0.4033, -0.4779, -0.5759],\n",
      "        [-0.6551, -0.4546, -0.4906, -0.7768]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.7967e-04.\n",
      "tensor([[-0.1954, -0.1015, -0.1275, -0.5064],\n",
      "        [-0.6559, -0.1171, -0.2234, -0.9740],\n",
      "        [-1.0307, -0.1828, -0.1082, -1.7341],\n",
      "        [-0.2682, -0.3120, -0.3279, -0.3169],\n",
      "        [-0.5701, -0.2959, -0.4474, -0.8382],\n",
      "        [-1.0741, -0.5438, -0.5327, -1.3584],\n",
      "        [-0.2371, -0.3312, -0.2913, -0.3634],\n",
      "        [-0.3069, -0.1093, -0.2071, -0.6882],\n",
      "        [-0.4237, -0.4506, -0.4636, -0.5345],\n",
      "        [-0.0426, -0.0814, -0.0433, -0.2153],\n",
      "        [-0.4425, -0.4178, -0.4726, -0.4404],\n",
      "        [-0.5131, -0.4534, -0.5610, -0.5967],\n",
      "        [-0.0946, -0.1357, -0.1033, -0.3411],\n",
      "        [-0.4827, -0.4829, -0.5144, -0.5963],\n",
      "        [-0.4654, -0.4295, -0.4879, -0.4433],\n",
      "        [-0.0504, -0.1197, -0.0748, -0.3130]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.5050e-04.\n",
      "tensor([[-0.8147,  0.0147, -0.1105, -1.2560],\n",
      "        [-0.5488, -0.5010, -0.6521, -0.7018],\n",
      "        [-0.5831, -0.1671,  0.0532, -0.9116],\n",
      "        [-0.5021, -0.5172, -0.5843, -0.7184],\n",
      "        [-0.4781, -0.2170, -0.1341, -0.8469],\n",
      "        [-0.7260, -0.0667, -0.0690, -1.1165],\n",
      "        [-0.4897, -0.4750, -0.5861, -0.5887],\n",
      "        [-0.2363, -0.2867, -0.3132, -0.3231],\n",
      "        [-0.1547, -0.2409, -0.1941, -0.4023],\n",
      "        [-0.1012, -0.0556, -0.0894, -0.2675],\n",
      "        [-0.3916, -0.4310, -0.4124, -0.5502],\n",
      "        [-0.5128, -0.5456, -0.5825, -0.7599],\n",
      "        [-0.1053, -0.1529, -0.1484, -0.3687],\n",
      "        [-0.5304, -0.5468, -0.6187, -0.7519],\n",
      "        [-0.2740, -0.3338, -0.3131, -0.4565],\n",
      "        [-0.1527, -0.0781, -0.0912, -0.3977]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2133e-04.\n",
      "tensor([[-0.3009, -0.3422, -0.3332, -0.5277],\n",
      "        [-0.0878, -0.1022, -0.0396, -0.3038],\n",
      "        [-0.4094, -0.4742, -0.4441, -0.6921],\n",
      "        [-0.9798, -0.5747, -0.4262, -1.2905],\n",
      "        [-1.1801, -0.3237, -0.2914, -1.6629],\n",
      "        [-0.0843, -0.1643, -0.1122, -0.3409],\n",
      "        [-0.4739, -0.4916, -0.5056, -0.7233],\n",
      "        [-0.1731, -0.0747, -0.1151, -0.4153],\n",
      "        [-0.4048, -0.4824, -0.3873, -0.6428],\n",
      "        [-0.1859, -0.1805, -0.1545, -0.3961],\n",
      "        [-0.3615, -0.3705, -0.4047, -0.6114],\n",
      "        [-0.3948, -0.4614, -0.4639, -0.6407],\n",
      "        [-0.1006, -0.0693, -0.0693, -0.2788],\n",
      "        [-0.4453, -0.0978, -0.0884, -0.8318],\n",
      "        [-0.0790, -0.1470, -0.0795, -0.3562],\n",
      "        [-0.6528, -0.5385, -0.5279, -0.8915]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.3289e-05.\n",
      "tensor([[-0.1374, -0.0269, -0.1000, -0.2563],\n",
      "        [-0.0865, -0.1649, -0.0539, -0.4144],\n",
      "        [-0.1807, -0.0812, -0.1219, -0.3953],\n",
      "        [-0.2622, -0.4203, -0.2699, -0.6356],\n",
      "        [-0.3062, -0.3640, -0.3552, -0.5037],\n",
      "        [-0.3381, -0.4253, -0.3874, -0.5685],\n",
      "        [-0.4551, -0.4729, -0.5048, -0.6961],\n",
      "        [-0.3411, -0.4182, -0.3851, -0.5648],\n",
      "        [-0.1884, -0.0180, -0.1446, -0.3223],\n",
      "        [-0.1995, -0.3463, -0.1714, -0.5304],\n",
      "        [-1.6034, -0.2028,  0.2505, -1.9153],\n",
      "        [-0.3166, -0.3997, -0.3692, -0.5492],\n",
      "        [-0.0955, -0.1124, -0.1009, -0.2169],\n",
      "        [-0.3886, -0.5367, -0.4242, -0.7818],\n",
      "        [-0.0915, -0.1948, -0.0937, -0.4008],\n",
      "        [-0.4176, -0.5067, -0.4131, -0.7585]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.7442e-05.\n",
      "tensor([[-0.1990, -0.3076, -0.1006, -0.5152],\n",
      "        [-0.1601, -0.2925, -0.2086, -0.4182],\n",
      "        [-0.3686, -0.3743, -0.2338, -0.5606],\n",
      "        [-0.0924, -0.1550, -0.0813, -0.3391],\n",
      "        [-0.3815, -0.5660, -0.4609, -0.7333],\n",
      "        [-1.4866,  0.2159,  0.4403, -1.4775],\n",
      "        [-0.1940, -0.1562, -0.0764, -0.5155],\n",
      "        [-0.1569, -0.2583, -0.1153, -0.4599],\n",
      "        [-0.0704, -0.0840, -0.0641, -0.1996],\n",
      "        [-0.2988, -0.4119, -0.3884, -0.4954],\n",
      "        [-0.4909, -0.6463, -0.5370, -0.9076],\n",
      "        [-0.3262, -0.4265, -0.4182, -0.5154],\n",
      "        [-0.2090, -0.1532, -0.1042, -0.5515],\n",
      "        [-0.8136, -0.5768, -0.3113, -1.1450],\n",
      "        [-0.4758, -0.1712, -0.1707, -0.6477],\n",
      "        [-0.3359, -0.4600, -0.4286, -0.5325]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.4788e-05.\n",
      "tensor([[-0.4530, -0.6902, -0.5477, -0.7560],\n",
      "        [-0.4120, -0.6221, -0.4804, -0.7053],\n",
      "        [-0.1139, -0.2419, -0.1485, -0.2699],\n",
      "        [-0.1381, -0.1595, -0.1653, -0.2686],\n",
      "        [-2.0856,  0.1742,  0.6327, -1.9835],\n",
      "        [-0.0819, -0.0970, -0.0518, -0.2485],\n",
      "        [-0.1074, -0.1916, -0.0839, -0.3751],\n",
      "        [-0.7299, -0.3476, -0.1963, -0.8087],\n",
      "        [-0.3813, -0.5804, -0.4513, -0.6626],\n",
      "        [-0.0985, -0.1696, -0.0196, -0.4016],\n",
      "        [-0.1255, -0.1384, -0.0445, -0.3698],\n",
      "        [-0.1616, -0.2766, -0.1383, -0.4511],\n",
      "        [-0.4815, -0.7125, -0.5580, -0.8034],\n",
      "        [-0.2044, -0.1145, -0.1062, -0.4908],\n",
      "        [-0.3546, -0.5682, -0.4481, -0.6151],\n",
      "        [-0.0591, -0.1545,  0.0335, -0.3498]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.6195e-05.\n",
      "tensor([[-0.3481, -0.5511, -0.3973, -0.6535],\n",
      "        [-0.0419, -0.1719, -0.0447, -0.2825],\n",
      "        [-0.2730, -0.4879, -0.3188, -0.5682],\n",
      "        [-0.5257, -0.4573, -0.1879, -0.8555],\n",
      "        [-0.7776,  0.0340,  0.2404, -0.7454],\n",
      "        [-1.0547,  0.1269,  0.1809, -1.0437],\n",
      "        [-0.3132, -0.4709, -0.3723, -0.5613],\n",
      "        [-0.1455, -0.1174, -0.1667, -0.2343],\n",
      "        [-0.9763, -0.4493, -0.1291, -1.1591],\n",
      "        [-0.1450, -0.2616, -0.1411, -0.4108],\n",
      "        [-0.2257, -0.3631, -0.2786, -0.4234],\n",
      "        [-0.2315, -0.4356, -0.3222, -0.4656],\n",
      "        [-0.1640, -0.1716, -0.0931, -0.4458],\n",
      "        [-0.4408, -0.5763, -0.2967, -0.7218],\n",
      "        [-0.1703, -0.1198, -0.1152, -0.3851],\n",
      "        [-0.3044, -0.5193, -0.4044, -0.5728]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2380e-05.\n",
      "tensor([[-0.2925, -0.4910, -0.3724, -0.4327],\n",
      "        [-0.2419, -0.4687, -0.2850, -0.4789],\n",
      "        [-0.3166, -0.5342, -0.3497, -0.5781],\n",
      "        [-1.1161, -0.1481,  0.3079, -1.0862],\n",
      "        [-1.1491,  0.0448,  0.1583, -1.1058],\n",
      "        [-0.1372, -0.1087, -0.1283, -0.2628],\n",
      "        [-0.3570, -0.5991, -0.4141, -0.6266],\n",
      "        [-0.1183, -0.1538, -0.0125, -0.4302],\n",
      "        [-0.6987, -0.1695,  0.1049, -0.8009],\n",
      "        [-0.1808, -0.1521, -0.0921, -0.4747],\n",
      "        [-0.2499, -0.4641, -0.3110, -0.4559],\n",
      "        [-0.3035, -0.4548, -0.3874, -0.4521],\n",
      "        [-0.3059, -0.5209, -0.3386, -0.5684],\n",
      "        [-0.1376, -0.1637, -0.0209, -0.4404],\n",
      "        [-0.3112, -0.4738, -0.3936, -0.4589],\n",
      "        [-0.3341, -0.4964, -0.4007, -0.5713]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 3.8726e-06.\n",
      "tensor([[-0.3014, -0.5355, -0.3622, -0.5595],\n",
      "        [-0.2108, -0.1844, -0.0611, -0.5712],\n",
      "        [-0.3373, -0.4771, -0.4259, -0.4695],\n",
      "        [-0.0911, -0.2048, -0.1102, -0.2839],\n",
      "        [-0.6661, -0.1114,  0.1287, -0.7835],\n",
      "        [-0.5317, -0.3320, -0.2602, -0.7224],\n",
      "        [-0.3321, -0.4567, -0.4192, -0.4937],\n",
      "        [-0.2232, -0.3641, -0.2729, -0.3461],\n",
      "        [-0.7465, -0.1944, -0.0381, -0.8111],\n",
      "        [-0.3116, -0.5638, -0.3472, -0.6145],\n",
      "        [-0.2023, -0.1626, -0.1746, -0.4741],\n",
      "        [-0.1286, -0.3266, -0.0864, -0.3496],\n",
      "        [-0.2881, -0.5184, -0.3525, -0.5510],\n",
      "        [-0.6287, -0.2186, -0.0480, -0.7565],\n",
      "        [-0.9125, -0.0959,  0.2254, -0.9591],\n",
      "        [-0.2803, -0.4822, -0.3428, -0.5016]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "tensor([[-0.1273, -0.1887, -0.0583, -0.3432],\n",
      "        [-0.1237, -0.1509, -0.0103, -0.3929],\n",
      "        [-0.1374, -0.3011, -0.0486, -0.4031],\n",
      "        [-0.1740, -0.3698, -0.2525, -0.3605],\n",
      "        [-0.3132, -0.4906, -0.3971, -0.4372],\n",
      "        [-0.4238, -0.5900, -0.5307, -0.5890],\n",
      "        [-0.1815, -0.1709, -0.0993, -0.4247],\n",
      "        [-0.2807, -0.5051, -0.3934, -0.4741],\n",
      "        [-0.2320, -0.2809, -0.2833, -0.3584],\n",
      "        [-0.6770, -0.7476, -0.6253, -0.9524],\n",
      "        [-0.3283, -0.5641, -0.4129, -0.5688],\n",
      "        [-0.5481, -0.2938, -0.1562, -0.6268],\n",
      "        [-1.6029,  0.0236,  0.2321, -1.4923],\n",
      "        [-0.1850, -0.3759, -0.2511, -0.3719],\n",
      "        [-0.1515, -0.2381, -0.0828, -0.4171],\n",
      "        [-0.0624, -0.1171, -0.0590, -0.1869]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 3.8726e-06.\n",
      "tensor([[-0.2822, -0.5201, -0.3863, -0.5153],\n",
      "        [-0.1823, -0.3402, -0.1311, -0.4321],\n",
      "        [-0.1801, -0.3941, -0.2778, -0.3559],\n",
      "        [-0.1412, -0.1264, -0.0486, -0.4002],\n",
      "        [-0.2380, -0.1782, -0.0067, -0.4650],\n",
      "        [-0.3101, -0.5490, -0.4030, -0.5605],\n",
      "        [-0.4136, -0.3493, -0.1959, -0.6651],\n",
      "        [-0.1610, -0.1313, -0.0667, -0.4357],\n",
      "        [-0.2532, -0.4610, -0.3618, -0.4556],\n",
      "        [-1.5881,  0.0971,  0.2893, -1.4738],\n",
      "        [-0.3166, -0.3751, -0.4285, -0.4161],\n",
      "        [-1.0002, -0.3313,  0.1397, -0.9838],\n",
      "        [-0.2504, -0.4856, -0.3340, -0.4774],\n",
      "        [-0.2346, -0.4718, -0.3474, -0.4397],\n",
      "        [-0.1385, -0.1965, -0.1347, -0.3566],\n",
      "        [-0.2878, -0.4760, -0.3944, -0.4984]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2380e-05.\n",
      "tensor([[-0.4349, -0.7397, -0.4661, -0.7607],\n",
      "        [-0.0744, -0.0331, -0.0219, -0.1639],\n",
      "        [-0.9574, -0.0479,  0.2423, -0.9186],\n",
      "        [-0.3570, -0.5120, -0.3531, -0.6416],\n",
      "        [-0.1849, -0.2613, -0.2383, -0.2959],\n",
      "        [-0.1858, -0.0811, -0.1078, -0.3568],\n",
      "        [-0.3798, -0.6523, -0.4467, -0.6504],\n",
      "        [-1.2697, -0.0161,  0.1165, -1.2976],\n",
      "        [-0.2543, -0.4237, -0.2938, -0.4536],\n",
      "        [-0.1359, -0.0783, -0.1144, -0.2557],\n",
      "        [-0.1174, -0.1784, -0.1445, -0.2043],\n",
      "        [-0.3344, -0.6032, -0.3975, -0.5666],\n",
      "        [-0.7686, -0.2535,  0.1037, -0.7998],\n",
      "        [-0.2452, -0.4901, -0.2968, -0.4783],\n",
      "        [-0.2532, -0.4418, -0.2398, -0.5237],\n",
      "        [-0.3636, -0.6329, -0.4251, -0.6455]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 2.6195e-05.\n",
      "tensor([[-0.6205, -0.1428, -0.0137, -0.6856],\n",
      "        [-0.1404, -0.0926, -0.1733, -0.2263],\n",
      "        [-0.1190, -0.2127, -0.1497, -0.2527],\n",
      "        [-0.3730, -0.5711, -0.4724, -0.5815],\n",
      "        [-0.1577, -0.1255, -0.1219, -0.3151],\n",
      "        [-0.3488, -0.6178, -0.3718, -0.6582],\n",
      "        [-1.2006,  0.0485,  0.2653, -1.0861],\n",
      "        [-0.1440, -0.2562, -0.1231, -0.3858],\n",
      "        [-0.7276, -0.2316,  0.1002, -0.8463],\n",
      "        [-0.3690, -0.5344, -0.3848, -0.6893],\n",
      "        [-0.3199, -0.6020, -0.4014, -0.6032],\n",
      "        [-0.3520, -0.6374, -0.4146, -0.6706],\n",
      "        [-0.3420, -0.3486, -0.3874, -0.4477],\n",
      "        [-0.1070, -0.1580, -0.0939, -0.1755],\n",
      "        [-0.5477, -0.4345, -0.0045, -0.8011],\n",
      "        [-0.3435, -0.5579, -0.4351, -0.5732]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 4.4788e-05.\n",
      "tensor([[-0.3956, -0.5612, -0.4493, -0.6909],\n",
      "        [-0.2736, -0.4448, -0.4058, -0.3668],\n",
      "        [-0.2786, -0.5273, -0.3743, -0.5161],\n",
      "        [-0.2582, -0.1600, -0.2948, -0.3440],\n",
      "        [-0.2794, -0.5619, -0.4053, -0.5177],\n",
      "        [-0.8005, -0.5300, -0.2055, -0.9534],\n",
      "        [-0.6363, -0.0589,  0.0403, -0.6999],\n",
      "        [-0.2552, -0.4593, -0.1971, -0.5266],\n",
      "        [-0.1498, -0.1879, -0.0984, -0.4161],\n",
      "        [-0.3185, -0.5320, -0.3701, -0.6014],\n",
      "        [-0.1597, -0.2278, -0.0755, -0.4745],\n",
      "        [-0.9119, -0.0980,  0.0218, -0.9791],\n",
      "        [-0.2023, -0.1169, -0.1311, -0.4444],\n",
      "        [-0.1439, -0.2272, -0.1695, -0.2968],\n",
      "        [-0.4317, -0.4942, -0.1588, -0.6353],\n",
      "        [-0.9843, -0.2607, -0.0180, -1.0086]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 6.7442e-05.\n",
      "tensor([[-0.0349, -0.1296, -0.0299, -0.1629],\n",
      "        [-1.0541, -0.0146, -0.1697, -1.1310],\n",
      "        [-0.1915, -0.4036, -0.2845, -0.4256],\n",
      "        [-0.8344, -0.2199,  0.0999, -0.8881],\n",
      "        [-1.0319, -0.0418, -0.0575, -1.0566],\n",
      "        [-0.1479, -0.1658, -0.0351, -0.3665],\n",
      "        [-0.6625, -0.2168,  0.0280, -0.8172],\n",
      "        [-0.2735, -0.4798, -0.4249, -0.4055],\n",
      "        [-0.6725, -0.7652, -0.3707, -0.9078],\n",
      "        [-0.2827, -0.6083, -0.4251, -0.5353],\n",
      "        [-0.1778, -0.3592, -0.2859, -0.3351],\n",
      "        [-0.2685, -0.0997, -0.1936, -0.5005],\n",
      "        [-0.1661, -0.2348, -0.1190, -0.4583],\n",
      "        [-0.1773, -0.4261, -0.2579, -0.4189],\n",
      "        [-0.3059, -0.6295, -0.4720, -0.5441],\n",
      "        [-0.3275, -0.6230, -0.4400, -0.5523]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 9.3289e-05.\n",
      "tensor([[-0.2132, -0.4731, -0.3911, -0.3974],\n",
      "        [-0.2177, -0.4793, -0.4087, -0.3939],\n",
      "        [-0.7382, -0.2578,  0.0436, -0.9503],\n",
      "        [-0.2982, -0.4800, -0.4692, -0.4781],\n",
      "        [-0.1895, -0.3267, -0.3375, -0.2688],\n",
      "        [-0.9063, -0.2158, -0.2576, -1.0832],\n",
      "        [-0.0660, -0.0658, -0.0259, -0.1818],\n",
      "        [-1.0683, -0.0504,  0.1315, -1.0531],\n",
      "        [-0.2406, -0.4801, -0.3386, -0.5166],\n",
      "        [-0.1971, -0.4706, -0.3832, -0.3727],\n",
      "        [-0.1944, -0.2982, -0.2845, -0.4050],\n",
      "        [-0.3249, -0.5923, -0.4891, -0.5846],\n",
      "        [-0.7233, -0.3737,  0.0898, -0.9527]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]], device='cuda:0')\n",
      "Adjusting learning rate of group 0 to 1.2133e-04.\n",
      "{'train_loss': tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(0.0658), 'lr': 0.00012133399685858874}\n",
      "Val Loss: tensor(0.1731, device='cuda:0')\n",
      "CNN Validation Score: tensor(0.1816)\n",
      "Saving\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "#Train Correspong Supervised CNN\n",
    "print('Fine tunning Cov-T')\n",
    "model_cnn.fc=nn.Linear(in_features=2048, out_features=4, bias=True)\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "val_metric=MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_cnn.train()\n",
    "best=0\n",
    "best_val=0\n",
    "last_loss=math.inf\n",
    "writer = SummaryWriter()\n",
    "#change Epoch Here\n",
    "for epoch in range(1):\n",
    "    for images,label in train_loader:\n",
    "        model_cnn.train()\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model_cnn.to(device)\n",
    "        pred_ts=model_cnn(images)\n",
    "        print(pred_ts)\n",
    "        print(label)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "    train_score=metric.compute()\n",
    "    logs = {'train_loss': loss, 'Recall': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "    writer.add_scalar(\"Supervised-CNN Loss/train\", loss, epoch)\n",
    "    writer.add_scalar(\"Supervised-CNN Recall/train\", train_score, epoch)\n",
    "   \n",
    "    print(logs)\n",
    "    if best < train_score:\n",
    "        with torch.no_grad():\n",
    "            best=train_score\n",
    "            model_cnn.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_cnn.to(device)\n",
    "                pred_ts=model_cnn(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('CNN Validation Score:',val_score)\n",
    "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                    \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_cnn,\n",
    "                        './CASS-CNN-part-ft.pt')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tunning Cov-T\n",
      "Adjusting learning rate of group 0 to 3.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.9713e-04.\n",
      "Adjusting learning rate of group 0 to 2.8862e-04.\n",
      "Adjusting learning rate of group 0 to 2.7480e-04.\n",
      "Adjusting learning rate of group 0 to 2.5621e-04.\n",
      "Adjusting learning rate of group 0 to 2.3356e-04.\n",
      "Adjusting learning rate of group 0 to 2.0771e-04.\n",
      "Adjusting learning rate of group 0 to 1.7967e-04.\n",
      "Adjusting learning rate of group 0 to 1.5050e-04.\n",
      "Adjusting learning rate of group 0 to 1.2133e-04.\n",
      "Adjusting learning rate of group 0 to 9.3289e-05.\n",
      "Adjusting learning rate of group 0 to 6.7442e-05.\n",
      "Adjusting learning rate of group 0 to 4.4788e-05.\n",
      "Adjusting learning rate of group 0 to 2.6195e-05.\n",
      "Adjusting learning rate of group 0 to 1.2380e-05.\n",
      "Adjusting learning rate of group 0 to 3.8726e-06.\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n",
      "Adjusting learning rate of group 0 to 3.8726e-06.\n",
      "Adjusting learning rate of group 0 to 1.2380e-05.\n",
      "Adjusting learning rate of group 0 to 2.6195e-05.\n",
      "Adjusting learning rate of group 0 to 4.4788e-05.\n",
      "Adjusting learning rate of group 0 to 6.7442e-05.\n",
      "Adjusting learning rate of group 0 to 9.3289e-05.\n",
      "Adjusting learning rate of group 0 to 1.2133e-04.\n",
      "Adjusting learning rate of group 0 to 1.5050e-04.\n",
      "Adjusting learning rate of group 0 to 1.7967e-04.\n",
      "Adjusting learning rate of group 0 to 2.0771e-04.\n",
      "Adjusting learning rate of group 0 to 2.3356e-04.\n",
      "Adjusting learning rate of group 0 to 2.5621e-04.\n",
      "Adjusting learning rate of group 0 to 2.7480e-04.\n",
      "{'train_loss': tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>), 'Recall': tensor(0.3158), 'lr': 0.0002748047070392304}\n",
      "Val Loss: tensor(0.3427, device='cuda:0')\n",
      "ViT Validation Score: tensor(0.2677)\n",
      "Saving\n"
     ]
    }
   ],
   "source": [
    "model_vit=torch.load('cass-r50-vit-isic.pt')\n",
    "#Train Correspong Supervised Vit\n",
    "print('Fine tunning Cov-T')\n",
    "model_vit.head=nn.Linear(in_features=768, out_features=4, bias=True)\n",
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model_vit.train()\n",
    "val_metric=MyF1Score(cfg)\n",
    "writer = SummaryWriter()\n",
    "from torch.autograd import Variable\n",
    "best=0\n",
    "best_val=0\n",
    "last_loss=math.inf\n",
    "#Number of Epoches Change here\n",
    "for epoch in range(1):\n",
    "    for images,label in train_loader:\n",
    "        model_vit.train()\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model_vit.to(device)\n",
    "        pred_ts=model_vit(images)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "    train_score=metric.compute()\n",
    "    logs = {'train_loss': loss, 'Recall': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "    writer.add_scalar(\"Supervised-ViT Loss/train\", loss, epoch)\n",
    "    writer.add_scalar(\"Supervised-ViT Recall/train\", train_score, epoch)\n",
    "   \n",
    "    print(logs)\n",
    "    if best < train_score:\n",
    "        with torch.no_grad():\n",
    "            best=train_score\n",
    "            model_vit.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_vit.to(device)\n",
    "                pred_ts=model_vit(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('ViT Validation Score:',val_score)\n",
    "            writer.add_scalar(\"ViT Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                    \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_vit,\n",
    "                        './CASS-ViT-part-ft.pt')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # data path\n",
    "    data_path  = 'BrainMRI/testing.csv'\n",
    "    train_imgs_dir  = 'BrainMRI/Testing'\n",
    "    # model info\n",
    "    # label info\n",
    "    label_num2str = {0: 'glioma',\n",
    "                     1: 'pituitary',\n",
    "                     2:'notumor',\n",
    "                     3:'meningioma'\n",
    "                     }\n",
    "    label_str2num = {'glioma': 0,\n",
    "                     'pituitary':1,\n",
    "                     'notumor':2,\n",
    "                     'meningioma':3\n",
    "                     }\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight = [0.2, 0.5970802919708029, 1.0, 0.25255474452554744] # copy the cls_weight from previous step or just use the variable\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_384'\n",
    "    seed = 77\n",
    "    num_classes = 4\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, cfg, img_names: list, labels: list, transform=None):\n",
    "        self.img_dir = cfg.train_imgs_dir\n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_names[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_ts = self.transform(img)\n",
    "        label_ts = self.labels[idx]\n",
    "        return img_ts, label_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        #f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        rec = self.tp/(self.tp + self.fn)\n",
    "        return rec\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            if len(lb_str_list) == 0:\n",
    "                lb_str_list = ['healthy']\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 77\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(77)\n",
    "cfg=CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same Image Mean and STD as in the training steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_IMAGE_MEAN = (0.1857, 0.1857, 0.1858)\n",
    "DATASET_IMAGE_STD = (0.2018, 0.2018, 0.2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomAffine(degrees=10),], p=0.3),\n",
    "                                tsfm.RandomVerticalFlip(p=0.3),\n",
    "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('BrainMRI/testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notumor       405\n",
       "meningioma    306\n",
       "glioma        300\n",
       "pituitary     300\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_names: list = df['path'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_labels_ts = []\n",
    "for tmp_lb in df['target']:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    label_num=CFG.label_str2num.get(tmp_lb)\n",
    "    k=int(label_num)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_ts.append(tmp_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(CFG, all_img_names,all_img_labels_ts, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('./CASS-ViT-part-ft.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 3.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdeng11/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (MyF1Score). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': tensor(0.2133, device='cuda:0'), 'Recall': tensor(0.3094), 'lr': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "metric = MyF1Score(cfg)\n",
    "val_metric=MyF1Score(cfg)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr,verbose=True)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images,label in test_loader:\n",
    "        images = images.to(device)\n",
    "        label = label.to(device)\n",
    "        model.to(device)\n",
    "        pred_ts=model(images)\n",
    "        loss = criterion(pred_ts, label)\n",
    "        score = metric(pred_ts, label)\n",
    "test_score=metric.compute()\n",
    "logs = {'train_loss': loss, 'Recall': test_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper\n",
    "\n",
    "#Not applicable in draft since we are running with only one epoche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "We set up a local test environment to test whether our CASS implementation was capable of running successfully for a single epoch with a batch size of 16.  This was successful and a single epoch took about 30 minutes using a GPU with 22GB RAM.  After training for one epoch on the Brain MRI dataset, we achieved a Recall value of 0.3 which is only slightly better than a naive random baseline.  This was to be expected given the extremely limited training duration and only served as a sanity check that our implementation was functioning correctly.  We believe this is a strong indicator that this paper is reproducible.  \n",
    "\n",
    "Our next step is to scale up our training process and run the model for a sufficient number of epochs (50+) in order to meaningfully compare our results with those reported by the authors.  This is a potential challenge given that our GPU has less than half of the memory of the GPU used by the authors.  This will likely restrict our batch size to less than 20, which is not ideal.  To address this issue, weâ€™re considering setting up a cloud environment with more powerful GPUs so that we can reduce the overall training time and use a larger batch size.  We will assess the feasibility and cost effectiveness of that option in the coming days and make a decision based on the available resources and time constraints.\n",
    "\n",
    "We also plan to evaluate using other relevant metrics, like F1 score.  If time permits, we may extend our study to another dataset, that being the DermoFIT dataset which is a private, paid dataset.  We have applied for access and are still waiting for a decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.  Singh, P. and Cirrone, J., â€œEfficient Representation Learning for Healthcare with Cross-Architectural Self-Supervisionâ€, <i>arXiv e-prints</i>, 2023. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
